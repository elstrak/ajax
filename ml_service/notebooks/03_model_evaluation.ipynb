{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version used by PyTorch: 11.8\n",
      "Number of GPUs: 1\n",
      "Current CUDA device: 0\n",
      "Device name: NVIDIA GeForce RTX 4070 SUPER\n",
      "Added to sys.path: e:\\Code\\diplom\\ml_service\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crazy\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Main config base dir: E:\\Code\\diplom\\ml_service\n",
      "Pytorch device in use: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import os\n",
    "import pathlib # Для работы с путями\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version used by PyTorch: {torch.version.cuda}\") # Версия CUDA, с которой скомпилирован PyTorch\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available. PyTorch is using CPU.\")\n",
    "# --------------------------------------------------------------------------------\n",
    "# Добавление пути к корневой директории проекта (ml_service)\n",
    "module_path = os.path.abspath(os.path.join('.')) # Если ноутбук в ml_service/notebooks/\n",
    "# Проверяем, что мы в правильной директории или корректируем путь\n",
    "# Предполагаем, что ноутбук запускается из директории ml_service/notebooks/\n",
    "# Тогда родительская директория (ml_service) должна быть в sys.path\n",
    "if os.path.basename(module_path) == \"notebooks\":\n",
    "    module_path = os.path.dirname(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(f\"Added to sys.path: {module_path}\")\n",
    "\n",
    "\n",
    "from config import main_config\n",
    "from src.modeling.dataset import ContractChunkDataset\n",
    "from src.modeling.models import ContractVulnerabilityClassifier\n",
    "from src.modeling.trainer import train_model, DEVICE # DEVICE импортируется из trainer\n",
    "\n",
    "print(f\"Main config base dir: {main_config.BASE_DIR}\")\n",
    "print(f\"Pytorch device in use: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels (vulnerability types): 7\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# 2. Конфигурация эксперимента (можно вынести в отдельную ячейку или params.yaml для DVC)\n",
    "# --------------------------------------------------------------------------------\n",
    "# Имя токенизатора, которое использовалось при создании чанков\n",
    "TOKENIZER_NAME = \"microsoft/codebert-base\" \n",
    "TOKENIZER_NAME_FOR_PATH = TOKENIZER_NAME.replace('/', '_')\n",
    "\n",
    "# Параметры обучения\n",
    "NUM_EPOCHS = 5 # Для начала можно небольшое количество, потом увеличить\n",
    "LEARNING_RATE = 2e-5 # Типичное значение для fine-tuning\n",
    "BATCH_SIZE = 16 # Должно соответствовать тому, что может выдержать ваша GPU/CPU память\n",
    "# Если у вас мало VRAM, попробуйте BATCH_SIZE = 8 или 4\n",
    "\n",
    "# Путь для сохранения лучшей модели\n",
    "MODEL_FILENAME = f\"vuln_classifier_{TOKENIZER_NAME_FOR_PATH}_chunks_best.pt\"\n",
    "MODEL_SAVE_PATH = main_config.MODEL_DIR / MODEL_FILENAME\n",
    "main_config.MODEL_DIR.mkdir(parents=True, exist_ok=True) # Создаем директорию, если ее нет\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = main_config.EXPERIMENT_NAME\n",
    "\n",
    "# Определяем количество типов уязвимостей из конфига\n",
    "NUM_LABELS = len(main_config.VULNERABILITY_COUNT_COLUMNS)\n",
    "print(f\"Number of labels (vulnerability types): {NUM_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debugging File Paths ---\n",
      "Looking for TRAIN chunks at: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_chunks_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Looking for TRAIN labels at: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_chunk_labels_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Looking for TEST (validation) chunks at: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\test_chunks_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Looking for TEST (validation) labels at: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\test_chunk_labels_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "File suffix being used: t4096_c512_o64_microsoft_codebert-base.pt\n",
      "  Based on MAX_TOTAL_TOKENS: 4096\n",
      "  Based on MODEL_CHUNK_SIZE: 512\n",
      "  Based on CHUNK_OVERLAP: 64\n",
      "  Based on TOKENIZER_NAME_FOR_PATH: 'microsoft_codebert-base'\n",
      "--- End Debugging File Paths ---\n",
      "\n",
      "Loading chunk list from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_chunks_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Loading labels from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_chunk_labels_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Loading original indices from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_original_indices_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Dataset loaded. Number of chunks: 154231. Labels shape: torch.Size([154231, 7])\n",
      "\n",
      "Train dataset loaded. Length: 154231\n",
      "Loading chunk list from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\test_chunks_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Loading labels from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\test_chunk_labels_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Loading original indices from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\test_original_indices_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Dataset loaded. Number of chunks: 38703. Labels shape: torch.Size([38703, 7])\n",
      "Validation (using test set) dataset loaded. Length: 38703\n",
      "Train DataLoader created. Batches per epoch: ~9640\n",
      "Validation DataLoader created. Batches per epoch: ~2419\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# 3. Загрузка Dataset'ов и DataLoader'ов\n",
    "# --------------------------------------------------------------------------------\n",
    "PROCESSED_DATA_DIR_CHUNKS = main_config.PROCESSED_DATA_DIR / \"chunked_data\"\n",
    "file_suffix = (f\"t{main_config.MAX_TOTAL_TOKENS}_c{main_config.MODEL_CHUNK_SIZE}\"\n",
    "               f\"_o{main_config.CHUNK_OVERLAP}_{TOKENIZER_NAME_FOR_PATH}.pt\")\n",
    "\n",
    "PATH_TRAIN_CHUNKS = PROCESSED_DATA_DIR_CHUNKS / f\"train_chunks_{file_suffix}\"\n",
    "PATH_TRAIN_CHUNK_LABELS = PROCESSED_DATA_DIR_CHUNKS / f\"train_chunk_labels_{file_suffix}\"\n",
    "PATH_TRAIN_ORIGINAL_INDICES = PROCESSED_DATA_DIR_CHUNKS / f\"train_original_indices_{file_suffix}\"\n",
    "\n",
    "PATH_TEST_CHUNKS = PROCESSED_DATA_DIR_CHUNKS / f\"test_chunks_{file_suffix}\"\n",
    "PATH_TEST_CHUNK_LABELS = PROCESSED_DATA_DIR_CHUNKS / f\"test_chunk_labels_{file_suffix}\"\n",
    "PATH_TEST_ORIGINAL_INDICES = PROCESSED_DATA_DIR_CHUNKS / f\"test_original_indices_{file_suffix}\"\n",
    "\n",
    "# --- ОТЛАДОЧНЫЙ ВЫВОД ---\n",
    "print(\"\\n--- Debugging File Paths ---\")\n",
    "print(f\"Looking for TRAIN chunks at: {PATH_TRAIN_CHUNKS}\")\n",
    "print(f\"Looking for TRAIN labels at: {PATH_TRAIN_CHUNK_LABELS}\")\n",
    "print(f\"Looking for TEST (validation) chunks at: {PATH_TEST_CHUNKS}\")\n",
    "print(f\"Looking for TEST (validation) labels at: {PATH_TEST_CHUNK_LABELS}\")\n",
    "print(f\"File suffix being used: {file_suffix}\")\n",
    "print(f\"  Based on MAX_TOTAL_TOKENS: {main_config.MAX_TOTAL_TOKENS}\")\n",
    "print(f\"  Based on MODEL_CHUNK_SIZE: {main_config.MODEL_CHUNK_SIZE}\")\n",
    "print(f\"  Based on CHUNK_OVERLAP: {main_config.CHUNK_OVERLAP}\")\n",
    "print(f\"  Based on TOKENIZER_NAME_FOR_PATH: '{TOKENIZER_NAME_FOR_PATH}'\")\n",
    "print(\"--- End Debugging File Paths ---\\n\")\n",
    "# --- КОНЕЦ ОТЛАДОЧНОГО ВЫВОДА ---\n",
    "\n",
    "train_dataset = None\n",
    "val_dataset = None # Используем test_dataset как валидационный для этого примера\n",
    "\n",
    "if PATH_TRAIN_CHUNKS.exists() and PATH_TRAIN_CHUNK_LABELS.exists():\n",
    "    train_dataset = ContractChunkDataset(\n",
    "        chunk_list_path=str(PATH_TRAIN_CHUNKS),\n",
    "        labels_path=str(PATH_TRAIN_CHUNK_LABELS),\n",
    "        original_indices_path=str(PATH_TRAIN_ORIGINAL_INDICES) if PATH_TRAIN_ORIGINAL_INDICES.exists() else None\n",
    "    )\n",
    "    print(f\"\\nTrain dataset loaded. Length: {len(train_dataset)}\")\n",
    "else:\n",
    "    print(f\"ERROR: Training data files for chunks not found. Searched at {PROCESSED_DATA_DIR_CHUNKS}\")\n",
    "    # assert False, \"Training data not found\" # Можно раскомментировать для прерывания\n",
    "\n",
    "if PATH_TEST_CHUNKS.exists() and PATH_TEST_CHUNK_LABELS.exists():\n",
    "    # В данном примере используем тестовый набор как валидационный.\n",
    "    # В идеале, нужен отдельный валидационный набор.\n",
    "    val_dataset = ContractChunkDataset( # Называем его val_dataset для функции train_model\n",
    "        chunk_list_path=str(PATH_TEST_CHUNKS),\n",
    "        labels_path=str(PATH_TEST_CHUNK_LABELS),\n",
    "        original_indices_path=str(PATH_TEST_ORIGINAL_INDICES) if PATH_TEST_ORIGINAL_INDICES.exists() else None\n",
    "    )\n",
    "    print(f\"Validation (using test set) dataset loaded. Length: {len(val_dataset)}\")\n",
    "else:\n",
    "    print(f\"ERROR: Test/Validation data files for chunks not found. Searched at {PROCESSED_DATA_DIR_CHUNKS}\")\n",
    "    # Дополнительный вывод для отладки, какие именно файлы не найдены\n",
    "    if not PATH_TEST_CHUNKS.exists():\n",
    "        print(f\"  Specifically, test chunks file '{PATH_TEST_CHUNKS.name}' was NOT found in the directory.\")\n",
    "    if not PATH_TEST_CHUNK_LABELS.exists():\n",
    "        print(f\"  Specifically, test chunk labels file '{PATH_TEST_CHUNK_LABELS.name}' was NOT found in the directory.\")\n",
    "        \n",
    "train_dataloader = None\n",
    "val_dataloader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    print(f\"Train DataLoader created. Batches per epoch: ~{len(train_dataloader)}\")\n",
    "if val_dataset:\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    print(f\"Validation DataLoader created. Batches per epoch: ~{len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with base: microsoft/codebert-base\n",
      "Hidden size: 768\n",
      "Number of labels: 7\n",
      "\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# 4. Инициализация модели\n",
    "# --------------------------------------------------------------------------------\n",
    "if train_dataloader and val_dataloader : # Только если данные загружены\n",
    "    model = ContractVulnerabilityClassifier(\n",
    "        base_model_name=TOKENIZER_NAME, # Используем тот же, что и для токенизатора\n",
    "        num_labels=NUM_LABELS,\n",
    "        dropout_rate=0.1 # Можно настроить\n",
    "    )\n",
    "    print(\"\\nModel initialized.\")\n",
    "else:\n",
    "    print(\"\\nSkipping model initialization and training due to missing data.\")\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n",
      "MLflow Run ID: 95883e75e9534216a913a6957cda855b\n",
      "\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3602\n",
      "  Train accuracy_exact_match: 0.3928\n",
      "  Train f1_micro: 0.7918\n",
      "  Train precision_micro: 0.8245\n",
      "  Train recall_micro: 0.7616\n",
      "  Train f1_macro: 0.6492\n",
      "  Train precision_macro: 0.7956\n",
      "  Train recall_macro: 0.5986\n",
      "  Train roc_auc_macro: 0.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3284\n",
      "  Validation accuracy_exact_match: 0.4481\n",
      "  Validation f1_micro: 0.8048\n",
      "  Validation precision_micro: 0.8801\n",
      "  Validation recall_micro: 0.7414\n",
      "  Validation f1_macro: 0.7077\n",
      "  Validation precision_macro: 0.8461\n",
      "  Validation recall_macro: 0.6265\n",
      "  Validation roc_auc_macro: 0.8899\n",
      "New best validation F1-macro: 0.7077. Saving model...\n",
      "\n",
      "--- Epoch 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2822\n",
      "  Train accuracy_exact_match: 0.4993\n",
      "  Train f1_micro: 0.8456\n",
      "  Train precision_micro: 0.8729\n",
      "  Train recall_micro: 0.8200\n",
      "  Train f1_macro: 0.7609\n",
      "  Train precision_macro: 0.8471\n",
      "  Train recall_macro: 0.7085\n",
      "  Train roc_auc_macro: 0.9148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3058\n",
      "  Validation accuracy_exact_match: 0.4985\n",
      "  Validation f1_micro: 0.8318\n",
      "  Validation precision_micro: 0.8791\n",
      "  Validation recall_micro: 0.7894\n",
      "  Validation f1_macro: 0.7539\n",
      "  Validation precision_macro: 0.8447\n",
      "  Validation recall_macro: 0.6929\n",
      "  Validation roc_auc_macro: 0.9066\n",
      "New best validation F1-macro: 0.7539. Saving model...\n",
      "\n",
      "--- Epoch 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2376\n",
      "  Train accuracy_exact_match: 0.5635\n",
      "  Train f1_micro: 0.8733\n",
      "  Train precision_micro: 0.8980\n",
      "  Train recall_micro: 0.8500\n",
      "  Train f1_macro: 0.8107\n",
      "  Train precision_macro: 0.8769\n",
      "  Train recall_macro: 0.7632\n",
      "  Train roc_auc_macro: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2976\n",
      "  Validation accuracy_exact_match: 0.5250\n",
      "  Validation f1_micro: 0.8448\n",
      "  Validation precision_micro: 0.8838\n",
      "  Validation recall_micro: 0.8091\n",
      "  Validation f1_macro: 0.7683\n",
      "  Validation precision_macro: 0.8579\n",
      "  Validation recall_macro: 0.7098\n",
      "  Validation roc_auc_macro: 0.9114\n",
      "New best validation F1-macro: 0.7683. Saving model...\n",
      "\n",
      "--- Epoch 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2058\n",
      "  Train accuracy_exact_match: 0.6148\n",
      "  Train f1_micro: 0.8928\n",
      "  Train precision_micro: 0.9152\n",
      "  Train recall_micro: 0.8716\n",
      "  Train f1_macro: 0.8424\n",
      "  Train precision_macro: 0.8966\n",
      "  Train recall_macro: 0.8001\n",
      "  Train roc_auc_macro: 0.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3007\n",
      "  Validation accuracy_exact_match: 0.5438\n",
      "  Validation f1_micro: 0.8526\n",
      "  Validation precision_micro: 0.8788\n",
      "  Validation recall_micro: 0.8279\n",
      "  Validation f1_macro: 0.7891\n",
      "  Validation precision_macro: 0.8436\n",
      "  Validation recall_macro: 0.7472\n",
      "  Validation roc_auc_macro: 0.9144\n",
      "New best validation F1-macro: 0.7891. Saving model...\n",
      "\n",
      "--- Epoch 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1812\n",
      "  Train accuracy_exact_match: 0.6543\n",
      "  Train f1_micro: 0.9065\n",
      "  Train precision_micro: 0.9267\n",
      "  Train recall_micro: 0.8871\n",
      "  Train f1_macro: 0.8642\n",
      "  Train precision_macro: 0.9108\n",
      "  Train recall_macro: 0.8260\n",
      "  Train roc_auc_macro: 0.9664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3080\n",
      "  Validation accuracy_exact_match: 0.5600\n",
      "  Validation f1_micro: 0.8593\n",
      "  Validation precision_micro: 0.8792\n",
      "  Validation recall_micro: 0.8402\n",
      "  Validation f1_macro: 0.7955\n",
      "  Validation precision_macro: 0.8448\n",
      "  Validation recall_macro: 0.7583\n",
      "  Validation roc_auc_macro: 0.9153\n",
      "New best validation F1-macro: 0.7955. Saving model...\n",
      "\n",
      "Training finished.\n",
      "\n",
      "--- Model Training Finished ---\n",
      "Best model saved to: E:\\Code\\diplom\\ml_service\\models\\vuln_classifier_microsoft_codebert-base_chunks_best.pt\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# 5. Запуск обучения\n",
    "# --------------------------------------------------------------------------------\n",
    "if model and train_dataloader and val_dataloader:\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    trained_model = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        model_save_path=str(MODEL_SAVE_PATH),\n",
    "        mlflow_experiment_name=MLFLOW_EXPERIMENT_NAME\n",
    "    )\n",
    "    print(\"\\n--- Model Training Finished ---\")\n",
    "    print(f\"Best model saved to: {MODEL_SAVE_PATH if MODEL_SAVE_PATH.exists() else 'MLflow artifacts'}\")\n",
    "else:\n",
    "    print(\"\\nSkipping training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

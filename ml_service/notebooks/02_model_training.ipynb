{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import os\n",
    "\n",
    "    # Добавление пути к src, если необходимо\n",
    "module_path = os.path.abspath(os.path.join('..')) \n",
    "if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "from config import main_config\n",
    "from src.modeling.dataset import ContractChunkDataset # Импортируем наш новый Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "    # Настройки (убедитесь, что они соответствуют сохраненным файлам)\n",
    "    # --------------------------------------------------------------------------------\n",
    "TOKENIZER_NAME = \"microsoft/codebert-base\" # Имя токенизатора, которое использовалось\n",
    "TOKENIZER_NAME_FOR_PATH = TOKENIZER_NAME.replace('/', '_') # Для формирования пути\n",
    "\n",
    "PROCESSED_DATA_DIR_CHUNKS = main_config.PROCESSED_DATA_DIR / \"chunked_data\"\n",
    "file_suffix = (f\"t{main_config.MAX_TOTAL_TOKENS}_c{main_config.MODEL_CHUNK_SIZE}\"\n",
    "                   f\"_o{main_config.CHUNK_OVERLAP}_{TOKENIZER_NAME_FOR_PATH}.pt\")\n",
    "\n",
    "PATH_TRAIN_CHUNKS = PROCESSED_DATA_DIR_CHUNKS / f\"train_chunks_{file_suffix}\"\n",
    "PATH_TRAIN_CHUNK_LABELS = PROCESSED_DATA_DIR_CHUNKS / f\"train_chunk_labels_{file_suffix}\"\n",
    "PATH_TRAIN_ORIGINAL_INDICES = PROCESSED_DATA_DIR_CHUNKS / f\"train_original_indices_{file_suffix}\"\n",
    "    \n",
    "PATH_TEST_CHUNKS = PROCESSED_DATA_DIR_CHUNKS / f\"test_chunks_{file_suffix}\"\n",
    "PATH_TEST_CHUNK_LABELS = PROCESSED_DATA_DIR_CHUNKS / f\"test_chunk_labels_{file_suffix}\"\n",
    "PATH_TEST_ORIGINAL_INDICES = PROCESSED_DATA_DIR_CHUNKS / f\"test_original_indices_{file_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk list from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_chunks_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Loading labels from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_chunk_labels_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Loading original indices from: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\train_original_indices_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "Dataset loaded. Number of chunks: 154231. Labels shape: torch.Size([154231, 7])\n",
      "\n",
      "Train dataset created. Length: 154231\n",
      "ERROR: Test data files for chunks not found. Searched for:\n",
      "  Chunks: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\test_chunks_t4096_c512_o64_microsoft_codebert-base.pt\n",
      "  Labels: E:\\Code\\diplom\\ml_service\\data\\processed\\chunked_data\\test_chunk_labels_t4096_c512_o64_microsoft_codebert-base.pt\n"
     ]
    }
   ],
   "source": [
    "# Создание экземпляров Dataset\n",
    "if PATH_TRAIN_CHUNKS.exists() and PATH_TRAIN_CHUNK_LABELS.exists():\n",
    "        train_dataset = ContractChunkDataset(\n",
    "            chunk_list_path=str(PATH_TRAIN_CHUNKS),\n",
    "            labels_path=str(PATH_TRAIN_CHUNK_LABELS),\n",
    "            original_indices_path=str(PATH_TRAIN_ORIGINAL_INDICES) if PATH_TRAIN_ORIGINAL_INDICES.exists() else None\n",
    "        )\n",
    "        print(f\"\\nTrain dataset created. Length: {len(train_dataset)}\")\n",
    "else:\n",
    "        print(f\"ERROR: Training data files for chunks not found. Searched for:\")\n",
    "        print(f\"  Chunks: {PATH_TRAIN_CHUNKS}\")\n",
    "        print(f\"  Labels: {PATH_TRAIN_CHUNK_LABELS}\")\n",
    "        train_dataset = None # или assert False\n",
    "\n",
    "if PATH_TEST_CHUNKS.exists() and PATH_TEST_CHUNK_LABELS.exists():\n",
    "        test_dataset = ContractChunkDataset(\n",
    "            chunk_list_path=str(PATH_TEST_CHUNKS),\n",
    "            labels_path=str(PATH_TEST_CHUNK_LABELS),\n",
    "            original_indices_path=str(PATH_TEST_ORIGINAL_INDICES) if PATH_TEST_ORIGINAL_INDICES.exists() else None\n",
    "        )\n",
    "        print(f\"Test dataset created. Length: {len(test_dataset)}\")\n",
    "else:\n",
    "        print(f\"ERROR: Test data files for chunks not found. Searched for:\")\n",
    "        print(f\"  Chunks: {PATH_TEST_CHUNKS}\")\n",
    "        print(f\"  Labels: {PATH_TEST_CHUNK_LABELS}\")\n",
    "        test_dataset = None # или assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader created. Batches per epoch: ~9640\n",
      "\n",
      "Sample batch from train_dataloader:\n",
      "  Input IDs batch shape: torch.Size([16, 512])\n",
      "  Attention Mask batch shape: torch.Size([16, 512])\n",
      "  Labels batch shape: torch.Size([16, 7])\n",
      "  Original Index batch shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Создание DataLoader'ов\n",
    "BATCH_SIZE = 16 # Можно настроить\n",
    "\n",
    "if train_dataset:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # num_workers=0 для Windows для начала\n",
    "        print(f\"Train DataLoader created. Batches per epoch: ~{len(train_dataloader)}\")\n",
    "if test_dataset:\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        print(f\"Test DataLoader created. Batches per epoch: ~{len(test_dataloader)}\")\n",
    "\n",
    "    # Проверка одного батча из train_dataloader\n",
    "if train_dataset and len(train_dataset) > 0:\n",
    "        try:\n",
    "            print(\"\\nSample batch from train_dataloader:\")\n",
    "            sample_train_batch = next(iter(train_dataloader))\n",
    "            print(f\"  Input IDs batch shape: {sample_train_batch['input_ids'].shape}\")\n",
    "            print(f\"  Attention Mask batch shape: {sample_train_batch['attention_mask'].shape}\")\n",
    "            print(f\"  Labels batch shape: {sample_train_batch['labels'].shape}\")\n",
    "            if 'original_index' in sample_train_batch:\n",
    "                print(f\"  Original Index batch shape: {sample_train_batch['original_index'].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching a batch from train_dataloader: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "elif train_dataset is None:\n",
    "         print(\"Train dataset not created, cannot fetch a sample batch.\")\n",
    "else:\n",
    "         print(\"Train dataset is empty, cannot fetch a sample batch.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
